services:
  ollama:
    image: ollama/ollama
    environment:
      OLLAMA_NUM_THREADS: 4
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_KEEP_ALIVE: "-5m"
      OLLAMA_FLASH_ATTENTION: "false"
    volumes:
      - ./data/ollama:/root/.ollama
    networks:
      - mduck-net

  download-model:
    image: ollama/ollama
    volumes:
      - ./data/ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    env_file:
      - .env
    command: "pull ${OLLAMA__MODEL}"
    depends_on:
      - ollama
    networks:
      - mduck-net

  mduck:
    image: ghcr.io/aatrubilin/mduck:latest
    ports:
      - "8000:8000"
    env_file:
      - .env
    networks:
      - mduck-net
    depends_on:
      download-model:
        condition: service_completed_successfully
    volumes:
      - ./data/logs:/var/log/mduck
    entrypoint: run-webhook
    command: >
      --host 0.0.0.0
      --port 8000
      --log-level info
      --log-format human
      --log-file /var/log/mduck/mduck.log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthcheck"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 5s

networks:
  mduck-net:
